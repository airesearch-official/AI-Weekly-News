# 🚀 AI Weekly News – October 2025
**New AI Edits Any Video, #1 Open Source Model, Infinite, 4K and Realtime AI videos, quantum chip**

📅 Published: October 27, 2025  
🎥 Watch the full video here:  
[![Watch on YouTube](https://img.youtube.com/vi/o4MwIVJ_QbU/0.jpg)](https://youtu.be/o4MwIVJ_QbU)

---

## 🔥 Overview
This week, the AI world was shaken by breakthroughs in everything from video editing and generation to quantum computing and open-source models. We're seeing new tools that can edit any video, generate infinite-length 4K footage in real-time, and a new open-source LLM that is rivaling GPT-level performance.

Here’s a full breakdown with source links and insights 👇

---

## ✂️ Ditto – AI Edits Any Video
- **Ditto** is a new, holistic framework for instruction-based video editing.
- It uses a novel data generation pipeline to create a massive dataset of over one million high-fidelity video editing examples.
- The trained model, **Editto**, can perform both global style changes and precise local edits, like object replacement and attribute changes, with remarkable consistency.
- 🔗 [Ditto GitHub](https://github.com/EzioBy/Ditto)

---

## 🧠 MiniMax-M2 – #1 Open Source Model
- **MiniMax-M2** is a new, powerful, and efficient Mixture-of-Experts (MoE) model that has been open-sourced.
- With 230 billion total parameters, it achieves performance that rivals top-tier proprietary models, making it a new king in the open-source space.
- It's designed for local deployment and is already being used by developers for a wide range of applications.
- 🤗 [MiniMax-M2 on Hugging Face](https://huggingface.co/MiniMaxAI/MiniMax-M2)

---

## 🎬 HoloCine – Cinematic Long Video Generation
- **HoloCine** is a framework that bridges the "narrative gap" in AI video by generating entire multi-shot scenes holistically.
- It ensures global consistency, develops a persistent memory for characters and scenes, and has an intuitive grasp of cinematic techniques.
- This marks a pivotal shift from generating isolated clips to automated filmmaking.
- 🔗 [HoloCine Project Page](https://holo-cine.github.io/)

---

## ⚛️ Google's Quantum Chip & Earth AI
- Google announced a major breakthrough with its **Willow quantum chip**, demonstrating the first-ever verifiable quantum advantage with the **Quantum Echoes** algorithm.
- This is a significant step towards real-world applications for quantum computers.
- In parallel, **Google Earth AI** is getting new updates, using Gemini's advanced reasoning to help with environmental monitoring and disaster response.
- 📖 [Read about Quantum Echoes](https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/)
- 📖 [Read about Earth AI](https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/)

---

## 🌍 Google Vista – Self-Improving Video Generation
- **Google Vista** is a novel multi-agent system that autonomously improves video generation by refining prompts in an iterative loop.
- It decomposes a user's idea into a structured plan and uses a pairwise tournament to identify the best video, which then informs the next generation round.
- This test-time self-improving agent significantly enhances video quality and alignment with user intent.
- 🔗 [Google Vista Project Page](https://g-vista.github.io/)

---

## 🌐 ChatGPT Atlas – The AI Browser
- OpenAI has unveiled **ChatGPT Atlas**, an AI-powered web browser built around its popular chatbot.
- Atlas aims to be a "super-assistant" that understands your world, comes with you across the web, and helps you complete tasks in the window where you are.
- This is a major move to challenge Google's dominance in the browser market.
- 📖 [Read the Announcement](https://openai.com/index/introducing-chatgpt-atlas/)

---

## 🪞 Hunyuan World Mirror
- Tencent has open-sourced **Hunyuan World Mirror**, a versatile, feed-forward model for comprehensive 3D world creation.
- It can generate immersive and playable 3D worlds from text prompts, images, or videos with incredible speed on a single GPU.
- This is a significant contribution to the open-source community for 3D content creation.
- 🔗 [Hunyuan World Mirror GitHub](https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror)

---

## ⚡ DyPE – Dynamic Position Extrapolation
- **DyPE (Dynamic Position Extrapolation)** is a new technique that enables pre-trained diffusion transformers to generate ultra-high-resolution images far beyond their training scale.
- It dynamically adjusts positional encodings during the denoising process to achieve faithful upsampling to 4K and even 8K resolutions.
- This is a breakthrough for generating extremely detailed images without the massive cost of training at those scales.
- 🔗 [DyPE Project Page](https://noamissachar.github.io/DyPE/)

---

## 🎨 Luma Ray 3 Annotation
- **Luma Ray 3**, the reasoning video model, now supports **visual annotation**.
- Users can draw on images to precisely specify layout, motion, and character interactions, giving them more granular control over the generated video.
- This feature makes Ray 3 feel more like a creative partner, understanding visual intent beyond just text prompts.
- 🌐 [Visit Luma Labs](https://lumalabs.ai/)

---

## ♾️ Stable Video Infinity
- **Stable Video Infinity (SVI)** is a new method for generating infinite-length videos.
- It can scale videos from a few seconds to unlimited durations with no additional inference cost.
- SVI is compatible with diverse conditions like audio, skeleton, and text streams, making it a versatile tool for long-form video creation.
- 🔗 [Stable Video Infinity GitHub](https://github.com/vita-epfl/Stable-Video-Infinity)

---

## ⏱️ Krea Realtime 14B
- Krea AI has released **Krea Realtime 14B**, an open-source, real-time video diffusion model.
- Distilled from a larger model, it allows users to guide and redirect video streams as they are being created.
- This represents a shift away from "one-shot" generation towards more fluid and controllable creative tools.
- 🤗 [Krea Realtime Video on Hugging Face](https://huggingface.co/krea/krea-realtime-video)

---

## 🖌️ Inpaint4Drag
- **Inpaint4Drag** is a new approach to drag-based image editing that repurposes inpainting models.
- It decomposes the editing task into bidirectional warping and image inpainting, making it extremely fast (nearly 600x faster than DragDiffusion).
- This method serves as a universal adapter for any inpainting model, inheriting future improvements automatically.
- 🔗 [Inpaint4Drag Project Page](https://visual-ai.github.io/inpaint4drag/)

---

## 🎭 SoftMimic
- **SoftMimic** is a new technique for creating soft, deformable avatars that can mimic human motion.
- It focuses on generating realistic secondary motions, like the jiggling of soft tissues, which is crucial for creating lifelike digital characters.
- This research is a step towards more believable and expressive virtual avatars.
- 🔗 [SoftMimic Project Page](https://gmargo11.github.io/softmimic/)

---

## 🌌 UltraGen
- **UltraGen** is a novel video generation framework for efficient, end-to-end native high-resolution video synthesis (1080P/2K/4K).
- It uses a hierarchical attention mechanism to overcome the computational bottleneck that makes high-resolution training impractical.
- This is a key development for generating professional-quality video directly from AI.
- 🔗 [UltraGen Project Page](https://sjtuplayer.github.io/projects/UltraGen/)

---

## 🧊 Nano3D
- **Nano3D** is a training-free approach for efficient and consistent 3D object editing.
- It addresses long-standing challenges in 3D editing by ensuring that unedited regions are preserved perfectly.
- The project also introduces a new large-scale 3D editing dataset with over 100,000 high-quality examples.
- 🔗 [Nano3D Project Page](https://jamesyjl.github.io/Nano3D/)

---

## 🎬 LTX-2
- Lightricks has introduced **LTX-2**, a next-generation multimodal AI foundation model for video production.
- It's a complete creative engine that delivers synchronized audio and 4K video at 48 fps, and can run on consumer-grade GPUs.
- LTX-2 represents a major leap in making high-end, end-to-end video production accessible.
- 📖 [Read the LTX-2 Blog Post](https://website.ltx.video/blog/introducing-ltx-2)

---

## 🚀 Wrap Up
This week was a testament to the relentless pace of AI innovation, with breakthroughs that will redefine creative workflows and scientific research.
- AI video tools are becoming more powerful, accessible, and controllable than ever before.
- The open-source community continues to push the boundaries with models that rival proprietary systems.
- Google is making massive strides in both the quantum realm and practical AI applications.

👉 What do you think? Which breakthrough is the most exciting — AI video editing, the new open-source king, or Google's quantum chip?

💬 Drop your thoughts in the video comments:
[Watch the full video on YouTube](https://youtu.be/o4MwIVJ_QbU)

---

## 🔗 Follow & Support
- 🐦 Twitter/X: [@airesearch_ai](https://x.com/airesearch_ai)  
- ☕ Support: [Ko-fi](https://ko-fi.com/airesearchs)  
- 🎥 Subscribe for more: [AI Research YouTube](https://www.youtube.com/@airesearchofficial/)

---

#AI #AINews #AIWeekly #ArtificialIntelligence #AIVideo #OpenSource #QuantumComputing #AIResearch #AITrends

👉 Browse all past episodes here: [AI Weekly News Archive](../../..)
