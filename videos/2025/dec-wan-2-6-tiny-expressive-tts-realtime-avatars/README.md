# ğŸš€ AI Weekly News â€“ December 2025
**Wan 2.6, Tiny Expressive TTS, Realtime Avatars, AI Controls Android, Crazy 3B Model â€“ HUGE AI NEWS**

ğŸ“… Published: December 16, 2025  
ğŸ¥ Watch the full video here:  
[![Watch on YouTube](https://img.youtube.com/vi/cs4qEZKtepY/0.jpg)](https://youtu.be/cs4qEZKtepY)

---

## ğŸ”¥ Overview
This week was stacked with major breakthroughs, from next-gen video models to tiny AIs that are beating massive systems. We saw major upgrades to one of the most exciting AI video models, ultra-small voice models with shockingly human emotion, and AI that can control Android devices. The pace of innovation is unreal.

Hereâ€™s a full breakdown with source links and insights ğŸ‘‡

---

## ğŸ—£ï¸ Chatterbox Turbo & Gemini TTS
- **Chatterbox Turbo** is a new, highly efficient 350M parameter text-to-speech model from Resemble AI that delivers high-quality, expressive speech with less compute.
- Google has also upgraded its **Gemini 2.5 Flash and Pro Text-to-Speech models** with better expressiveness, pacing, and multi-speaker capabilities.
- These releases are pushing the boundaries of real-time, human-sounding AI voice generation.
- ğŸ¤— [Chatterbox Turbo on Hugging Face](https://huggingface.co/ResembleAI/chatterbox-turbo)
- ğŸ“– [Gemini TTS Announcement](https://blog.google/technology/developers/gemini-2-5-text-to-speech/)

---

## ğŸ“± AutoGLM
- **AutoGLM** is a new series of foundation agents from Zhipu AI designed for the autonomous control of digital devices through GUIs.
- It can learn through autonomous environmental interactions, with a focus on web browser and Android scenarios.
- This is a major step towards creating practical foundation agents that can operate our devices for us.
- ğŸ”— [AutoGLM Project Page](https://xiao9905.github.io/AutoGLM/)

---

## ğŸ¬ Wan 2.6 & Wan-Move
- **Wan 2.6** is the latest and most powerful version of the Wan video model, with major upgrades to realism, portrait texture, and lighting.
- **Wan-Move** is a new framework for motion-controllable video generation, allowing for precise control over the movement in generated videos.
- These releases solidify Wan's position as a top-tier video generation platform.
- ğŸ“– [Wan 2.6 Introduction](https://wan.video/blog/wan2.6-introduction)
- ğŸ”— [Wan-Move GitHub](https://github.com/ali-vilab/Wan-Move)

---

## ğŸš— RealGen
- **RealGen** is a new retrieval-augmented generation framework for creating controllable, realistic, and generalizable traffic simulations.
- It's designed to address the challenge of generating complex and diverse behaviors among agents in autonomous vehicle simulations.
- This is a critical tool for developing and testing the next generation of autonomous vehicles.
- ğŸ”— [RealGen Project Page](https://yejy53.github.io/RealGen/)

---

## âš¡ TwinFlow
- **TwinFlow** is a new framework for training 1-step generative models that bypasses the need for fixed pretrained teacher models.
- It's a simple yet effective approach that is ideal for building large-scale, efficient models for tasks like text-to-image generation.
- This could significantly speed up the training and inference of large generative models.
- ğŸ”— [TwinFlow Project Page](https://zhenglin-cheng.com/twinflow/)

---

## ğŸ§  GPT-5.2
- OpenAI has released **GPT-5.2**, the latest model in the GPT-5 series, with significant improvements in general intelligence, long-context understanding, and agentic tool-calling.
- It sets a new state of the art in long-context reasoning, making it a powerful tool for deep document analysis and other complex tasks.
- This is another major leap forward for the world's most powerful language model.
- ğŸ“– [Read the Announcement](https://openai.com/index/introducing-gpt-5-2/)

---

## ğŸ¤– Nemotron-3-Nano & Devstral 2
- NVIDIA has released **Nemotron-3-Nano**, a family of small, efficient, and open models for reasoning and agent development.
- Mistral has launched **Devstral 2**, a state-of-the-art open model for code agents, along with the **Mistral Vibe CLI** for end-to-end code automation.
- These releases are a huge win for the open-source community, providing powerful, accessible models for a wide range of tasks.
- ğŸ“– [Nemotron-3-Nano Announcement](https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models)
- ğŸ“– [Devstral 2 Announcement](https://mistral.ai/news/devstral-2-vibe-cli)

---

## ğŸ¨ Qwen-Image-i2L & TurboDiffusion
- **Qwen-Image-i2L** is a new model for generating LoRA weights that preserve image content and detail, accelerating the training of custom LoRA models.
- **TurboDiffusion** is a new technique for accelerating diffusion models, achieving a 100-205x speedup in inference time.
- These are powerful new tools for anyone working with diffusion models for image generation.
- ğŸ¤— [Qwen-Image-i2L on Hugging Face](https://huggingface.co/DiffSynth-Studio/Qwen-Image-i2L)
- ğŸ”— [TurboDiffusion GitHub](https://github.com/thu-ml/TurboDiffusion)

---

## ğŸ’ƒ One-to-All Animation & NewBie Image
- **One-to-All Animation** is a new system for generating long-length, high-fidelity AI videos from a single reference image, built on fine-tuned Wan 2.1 models.
- **NewBie-Image-Exp0.1** is a new 3.5B parameter DiT model for text-to-image generation that is delivering impressive results.
- These projects are pushing the boundaries of what's possible with image and video animation.
- ğŸ”— [One-to-All Animation GitHub](https://github.com/ssj9596/One-to-All-Animation)
- ğŸ¤— [NewBie-Image on Hugging Face](https://huggingface.co/NewBie-AI/NewBie-image-Exp0.1)

---

## ğŸƒ MoCapAnything & Window Reflection Removal
- **MoCapAnything** is a new framework for unified 3D motion capture for arbitrary skeletons, making it possible to capture motion from any video and apply it to any rigged 3D asset.
- A new **Window Reflection Removal** tool from Huawei uses AI to eliminate reflections from photos taken through windows.
- These are powerful new tools for 3D content creation and photo editing.
- ğŸ”— [MoCapAnything Project Page](https://animotionlab.github.io/MoCapAnything/)
- ğŸ¤— [Window Reflection Removal on Hugging Face](https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web)

---

## ğŸ‘¤ PersonaLive & Saber
- **PersonaLive** is a real-time, streamable diffusion framework for generating infinite-length portrait animations on a single GPU.
- **Saber** is a new zero-shot framework for reference-to-video generation that doesn't require explicit R2V data.
- These are major breakthroughs for creating realistic, controllable avatars and video content.
- ğŸ”— [PersonaLive GitHub](https://github.com/GVCLab/PersonaLive)
- ğŸ”— [Saber Project Page](https://franciszzj.github.io/Saber/)

---

## âœï¸ EgoEdit & Light-X
- **EgoEdit** is a new framework for real-time egocentric video editing, with a focus on object substitution and removal.
- **Light-X** is a powerful AI-driven photo and video editing app with a wide range of tools for creators.
- These tools are making it easier than ever to edit and manipulate video content in real-time.
- ğŸ”— [EgoEdit Project Page](https://snap-research.github.io/EgoEdit/)
- ğŸŒ [Light-X Website](https://www.lightxeditor.com/)

---

## ğŸŒ StereoWorld 3D & OneStory
- **StereoWorld** is a new end-to-end framework for high-fidelity monocular-to-stereo video generation.
- **OneStory** is a new model for coherent multi-shot video generation with adaptive memory, enabling controllable and immersive long-form video storytelling.
- These are major advances in creating immersive 3D and narrative video content.
- ğŸ”— [StereoWorld Project Page](https://ke-xing.github.io/StereoWorld/)
- ğŸ”— [OneStory Project Page](https://zhaochongan.github.io/projects/OneStory/)

---

## ğŸ§  Nanbeige4-3B
- **Nanbeige4-3B-Thinking-2511** is a new 3B parameter model that is achieving performance comparable to much larger models.
- It's a testament to the power of efficient model design and training, proving that smaller models can still be incredibly capable.
- This is a huge win for the open-source community and for anyone who wants to run powerful AI models on consumer hardware.
- ğŸ¤— [Nanbeige4-3B on Hugging Face](https://huggingface.co/Nanbeige/Nanbeige4-3B-Thinking-2511)

---

## ğŸš€ Wrap Up
This week was a whirlwind of progress, with AI models becoming more capable, more efficient, and more accessible than ever before.
- Tiny models are punching way above their weight, delivering performance that rivals massive systems.
- Real-time avatars and video generation are becoming a reality, opening up new possibilities for interactive content.
- AI is moving beyond the screen and into the real world, with agents that can control our devices and simulations that can model complex systems.

ğŸ‘‰ What do you think? Which update will have the biggest impact â€” the 3B model, real-time avatars, or Wan 2.6?

ğŸ’¬ Drop your thoughts in the video comments:
[Watch the full video on YouTube](https://youtu.be/cs4qEZKtepY)

---

## ğŸ”— Follow & Support
- ğŸ¦ Twitter/X: [@airesearch_ai](https://x.com/airesearch_ai)  
- â˜• Support: [Ko-fi](https://ko-fi.com/airesearchs)  
- ğŸ¥ Subscribe for more: [AI Research YouTube](https://www.youtube.com/@airesearchofficial/)

---

#AI #AINews #AIWeekly #ArtificialIntelligence #Wan2.6 #TTS #Avatars #AndroidAI #OpenSource #AIResearch #AITrends

ğŸ‘‰ Browse all past episodes here: [AI Weekly News Archive](../../..)
