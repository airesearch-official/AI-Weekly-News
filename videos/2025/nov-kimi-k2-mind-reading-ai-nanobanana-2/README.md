# ğŸš€ AI Weekly News â€“ November 2025
**Kimi K2 Thinking, Mind-Reading AI, NanoBanana 2, Humanoids, Realtime Videos â€“ HUGE AI News**

ğŸ“… Published: November 11, 2025  
ğŸ¥ Watch the full video here:  
[![Watch on YouTube](https://img.youtube.com/vi/OxyL6wWqV4M/0.jpg)](https://youtu.be/OxyL6wWqV4M)

---

## ğŸ”¥ Overview
This week, the world of AI took another massive leap forward with mind-reading AI, next-generation open-source models, and hyper-realistic humanoids. We're seeing AI that can decode human thoughts, models that rival top-tier LLMs, and video generators that are pushing the boundaries of realism and real-time performance.

Hereâ€™s a full breakdown with source links and insights ğŸ‘‡

---

## ğŸ“¸ Qwen Image Camera Angle Control
- A new LoRA for **Qwen-Edit** allows for incredible camera angle control in image editing.
- Users can now move the camera up, down, left, or right, and even rotate it to get the perfect shot.
- This provides a new level of creative freedom, allowing for dynamic and precise adjustments to any image.
- ğŸ¤— [Try the LoRA on Hugging Face](https://huggingface.co/dx8152/Qwen-Edit-2509-Multiple-angles)

---

## ğŸ§  Kimi K2 Thinking
- Moonshot AI has released **Kimi K2 Thinking**, a powerful new variant of their K2 model tuned for deep reasoning and long-horizon tool use.
- It's an open-weights model with a 256k context window, designed for building complex AI agents that can plan, browse, and code.
- This is a major step forward for open-source agentic AI.
- ğŸ¤— [Kimi K2 Thinking on Hugging Face](https://huggingface.co/moonshotai/Kimi-K2-Thinking)

---

## ğŸ’¡ UniLumos â€“ Unified Relighting
- Alibaba DAMO Academy has proposed **UniLumos**, a unified relighting framework for both images and videos.
- It brings RGB-space geometry feedback into a flow-matching backbone, achieving superior relighting quality and physical consistency.
- This allows for fast, controllable, and physically-grounded relighting of any scene.
- ğŸ”— [UniLumos on Hugging Face](https://huggingface.co/Alibaba-DAMO-Academy/UniLumos)

---

## ğŸ¤– Gen-0 â€“ Foundation Model for Robotics
- Generalist AI has unveiled **GEN-0**, a 10B+ parameter foundation model for robots built on a new "Harmonic Reasoning" architecture.
- Trained on an unprecedented 270,000+ hours of real-world dexterous data, it exhibits strong scaling laws, meaning it gets smarter with more data and compute.
- This is a major breakthrough in building generalist robots that can learn and adapt in the physical world.
- ğŸ“– [Read the Announcement](https://generalistai.com/blog/nov-04-2025-GEN-0)

---

## ğŸ§µ BindWeave â€“ Subject-Consistent Video Generation
- **BindWeave** is a new framework for subject-consistent video generation, allowing users to create videos with single or multiple subjects.
- It uses a Multimodal Large Language Model (MLLM) to intelligently parse instructions and maintain subject consistency across frames.
- This is a significant improvement for creating narrative videos where characters and objects need to remain consistent.
- ğŸ”— [BindWeave Project Page](https://lzy-dot.github.io/BindWeave/)

---

## â™¾ï¸ Infinity by Bytedance
- Bytedance has released **Infinity**, a new autoregressive text-to-image model that is redefining the state of the art.
- It uses a bitwise token prediction framework and an infinite-vocabulary tokenizer to generate images with incredible detail and quality, matching or surpassing leading diffusion models.
- This is a new king in the autoregressive image generation space.
- ğŸ”— [Infinity Project Page](https://foundationvision.github.io/infinity.project/)

---

## ğŸŒ NanoBanana 2 (GEMPIX2)
- The next generation of Google's image model, **NanoBanana 2** (codenamed GEMPIX2), is on the horizon.
- It promises to deliver higher fidelity, 4K image generation, and seamless fusion of multiple reference images for complex creative compositions.
- This upgrade will significantly broaden the creative range of Gemini's multimodal capabilities.
- ğŸŒ [Learn More](https://nanobanana2.com/)

---

## ğŸŒ OlmoEarth
- The Allen Institute for AI (AI2) has released **OlmoEarth**, a state-of-the-art Earth observation foundation model.
- It's an end-to-end platform for scalable planetary intelligence, designed to help organizations go from raw satellite data to actionable insights.
- This open-source tool will accelerate missions related to climate, conservation, and disaster response.
- ğŸ“– [Read the Announcement](https://allenai.org/blog/olmoearth-models)

---

## ğŸ§  BrainIT â€“ Mind-Reading AI
- **BrainIT** is a groundbreaking new model that can reconstruct images from fMRI brain scans.
- It uses a brain-interaction decoding transformer to interpret neural patterns and generate visual representations of what a person is seeing.
- This is a major step towards non-invasive brain-computer interfaces and understanding the human mind.
- ğŸ”— [BrainIT Project Page](https://amitzalcher.github.io/Brain-IT/)

---

## ğŸŒŠ MotionStream â€“ Real-Time Interactive Video
- **MotionStream** is a real-time, long-duration video generation system with interactive motion controls.
- Running at ~30 FPS on a single GPU, it allows users to paint trajectories, control cameras, or transfer motion and see the results unfold in real-time.
- This unlocks new possibilities for interactive content generation and live performance.
- ğŸ”— [MotionStream Project Page](https://joonghyuk.com/motionstream-web/)

---

## ğŸ§˜ CALM â€“ Continuous Autoregressive Language Models
- **CALM (Continuous Autoregressive Language Models)** is a new paradigm that confronts the one-token-at-a-time bottleneck in LLMs.
- It compresses chunks of tokens into continuous vectors, increasing the semantic bandwidth of each generative step.
- This allows CALM to achieve the performance of strong discrete models with significantly less compute.
- ğŸ“– [Read the Blog Post](https://shaochenze.github.io/blog/2025/CALM/)

---

## ğŸ¤– Xpeng Iron Humanoids
- Xpeng has debuted its next-generation **'Iron' humanoid robot**, which is so realistic that many initially suspected it was a human in a suit.
- The robot features a hyper-realistic design and is powered by a new VLT AI model, with a commercial production goal set for 2026.
- This marks a major step toward creating truly lifelike humanoid robots for various applications.
- ğŸ“– [Read More](https://www.humanoidsdaily.com/feed/generalist-ai-unveils-gen-0-claims-scaling-laws-for-robotics-backed-by-270-000-hours-of-real-world-data)

---

## ğŸš€ Wrap Up
This week was a stunning showcase of AI's rapid evolution, from decoding our thoughts to building the next generation of robotic intelligence.
- Mind-reading AI is no longer science fiction.
- Open-source models are continuing their relentless march, rivaling the performance of the biggest players.
- Real-time video and hyper-realistic humanoids are here, and they're changing everything.

ğŸ‘‰ What do you think? Which breakthrough is the most mind-blowing â€” mind-reading AI, NanoBanana 2, or the new humanoids?

ğŸ’¬ Drop your thoughts in the video comments:
[Watch the full video on YouTube](https://youtu.be/9I8m1pqXIRU)

---

## ğŸ”— Follow & Support
- ğŸ¦ Twitter/X: [@airesearch_ai](https://x.com/airesearch_ai)  
- â˜• Support: [Ko-fi](https://ko-fi.com/airesearchs)  
- ğŸ¥ Subscribe for more: [AI Research YouTube](https://www.youtube.com/@airesearchofficial/)

---

#AI #AINews #AIWeekly #ArtificialIntelligence #MindReading #NanoBanana2 #Humanoid #OpenSource #AIResearch #AITrends

ğŸ‘‰ Browse all past episodes here: [AI Weekly News Archive](../../..)
