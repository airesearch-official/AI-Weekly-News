# 🚀 AI Weekly News – September 2025
**Veo 3 is FREE, AI Video With Audio, Animate Anyone, new LLM, Claude Chrome | Huge AI News**

📅 Published: September 1, 2025  
🎥 Watch the full video here:  
[![Watch on YouTube](https://img.youtube.com/vi/RSaeCq7G6Y4/0.jpg)](https://youtu.be/RSaeCq7G6Y4)

---

## 🔥 Overview
This week, the AI world saw massive updates across video generation, animation, and real-time voice synthesis. Google’s Veo 3 is now accessible with free credits, AI-generated videos can now include synchronized audio, and new models are animating still images into full characters. Plus, Anthropic brings Claude directly into your browser.

Here’s a full breakdown with source links and insights 👇

---

## 🎬 Wan-S2V – Audio-Driven Video
- **Wan-S2V** is an open-source model that generates cinematic, high-definition videos from audio inputs.
- It excels at creating realistic talking heads, singing animations, and scenes with environmental sounds.
- Built on an advanced VAE, it supports 720p resolution and can run on consumer-grade GPUs.
- 📖 [Read the project page](https://humanaigc.github.io/wan-s2v-webpage/)

---

## 🌐 Claude for Chrome
- Anthropic is piloting a **Claude browser extension** for Chrome.
- It allows users to instruct Claude to perform actions directly on websites, acting as a true browser assistant.
- Currently in a limited-access research preview for selected Claude Max plan users.
- 📖 [Read the announcement](https://www.anthropic.com/news/claude-for-chrome)

---

## 🔨 VoxHammer – 3D Editing
- **VoxHammer** is a novel, training-free approach for precise and coherent 3D model editing.
- It operates directly in the native latent space of structured 3D generative models.
- This method allows for targeted edits while preserving the unedited regions of a 3D model.
- 🔗 [VoxHammer Project Page](https://huanngzh.github.io/VoxHammer-Page/)

---

## 🎨 USO – Unified Style & Subject Generation
- Developed by ByteDance, **USO (Unified Style-Subject Optimized)** is a customization model for image generation.
- It unifies style transfer and subject-driven generation, allowing users to place a subject in a new scene or transfer a style while keeping the layout.
- It disentangles "content" and "style" for highly customized visual creation.
- 🔗 [USO GitHub](https://github.com/bytedance/USO)

---

## 🧭 CoMPaSS – Spatial Image Generation
- **CoMPaSS** enhances spatial understanding in text-to-image diffusion models.
- It allows for more precise control over the placement and relationships of objects in a generated image.
- Developed by researchers to address common issues with spatial reasoning in AI image generation.
- 🔗 [CoMPaSS Project Page](https://compass.blurgy.xyz/)

---

## 🔊 Pika – Video with Audio
- **Pika Labs** has updated its AI video generator to include **sound generation**.
- Users can now generate video clips complete with synchronized soundtracks, sound effects, and voices.
- This feature brings Pika closer to an all-in-one solution for short-form video content.
- 🌐 [Visit Pika.art](https://pika.art)

---

## ✨ Pixie-3D – Physics from Pixels
- **Pixie** is a novel AI model that can infer the physical properties of objects from a single image.
- It maps visual features to dense material fields, allowing for physics-based 3D modeling and simulation.
- This bridges the gap between 2D images and physically accurate 3D scenes.
- 🔗 [Pixie-3D Project Page](https://pixie-3d.github.io/)

---

## 🌊 Waver 1.0 – Unified Video Model
- **Waver 1.0** is a universal foundation model for both image and video generation.
- It excels at creating videos with complex motion and strong temporal consistency, supporting resolutions up to 1080p.
- It can generate narrative videos with multiple cohesive shots, a significant leap from single-scene generators.
- 🔗 [Waver GitHub](https://github.com/FoundationVision/Waver)

---

## 🕺 OmniHuman-1.5 – Expressive Avatars
- **OmniHuman-1.5** generates expressive, full-body character animations from a single image and a voice track.
- It simulates cognitive processes to produce animations that are coherent with the speech's rhythm, prosody, and semantic content.
- This model aims to instill an "active mind" in digital avatars, making them more lifelike.
- 🔗 [OmniHuman-1.5 Project Page](https://omnihuman-lab.github.io/v1_5/)

---

## 💸 Google Flow & Veo 3 – Free Credits
- Google is now offering free **AI Credits** for its **Flow** platform, which uses the powerful **Veo 3** video generation model.
- This makes one of the most advanced text-to-video models more accessible to creators.
- Users on certain Google One plans receive monthly credits to generate cinematic scenes and stories.
- 🌐 [About Google Flow](https://labs.google/flow/about)

---

## 📱 MiniCPM-V 4.5 – GPT-4o Level on Mobile
- **MiniCPM-V 4.5** is a highly efficient, 8B parameter multimodal model that achieves performance comparable to GPT-4o.
- It's designed to run on-device, bringing powerful vision-language capabilities to mobile and edge devices.
- The model excels at a wide range of tasks, including video understanding, with low computational overhead.
- 🔗 [MiniCPM-V GitHub](https://github.com/OpenBMB/MiniCPM-V)

---

## 🗣️ GPT Realtime – Conversational AI
- OpenAI has released **GPT Realtime**, an API designed for low-latency, real-time conversational voice agents.
- It uses a persistent WebSocket connection to stream audio, enabling more natural interactions and handling interruptions automatically.
- The API also supports image inputs and SIP for phone calling, expanding its use cases.
- 📖 [Read the announcement](https://openai.com/index/introducing-gpt-realtime/)

---

## 🎙️ VibeVoice TTS – Expressive, Long-Form Audio
- Microsoft has open-sourced **VibeVoice**, a text-to-speech (TTS) framework for generating expressive, multi-speaker conversational audio.
- It's designed for long-form content like podcasts and supports over 90 languages.
- The model addresses key challenges in scalability, speaker consistency, and natural turn-taking.
- 🔗 [VibeVoice GitHub](https://github.com/microsoft/VibeVoice)

---

## 🎶 HunyuanVideo-Foley – AI Sound Effects
- Developed by Tencent, **HunyuanVideo-Foley** is an AI model that generates high-fidelity Foley audio (sound effects) for videos.
- It uses a Text-Video-to-Audio (TV2A) framework to create sounds that are synchronized with the on-screen action.
- This brings a new level of realism to AI-generated video content.
- 🔗 [HunyuanVideo-Foley GitHub](https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley)

---
