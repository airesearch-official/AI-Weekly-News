# ğŸš€ AI Weekly News â€“ January 2026
**Open-Source King Is Back, Real AI Livestreamers, Realtime Speech, Video Model, ClawdBot â€“ HUGE AI**

ğŸ“… Published: January 27, 2026  
ğŸ¥ Watch the full video here:  
[![Watch on YouTube](https://img.youtube.com/vi/HlDYI_G4Kb0/0.jpg)](https://youtu.be/HlDYI_G4Kb0)

---

## ğŸ”¥ Overview
This weekâ€™s AI updates feel like a shift from demos to reality. We're seeing the open-source king make a comeback, real AI livestreamers running and interacting live, and realtime AI speech that finally feels natural. This isn't just about one release â€” it's a full AI news roundup showing how AI is moving into real-time, live, and autonomous use cases.

Hereâ€™s a full breakdown with source links and insights ğŸ‘‡

---

## ğŸ‘‘ Kimi K2.5 & Qwen3-Max-Thinking
- **Kimi K2.5** is a new native multimodal model from Moonshot AI with state-of-the-art coding and vision capabilities, and a self-directed agent swarm paradigm that can execute complex tasks with up to 100 sub-agents.
- **Qwen3-Max-Thinking** is the latest and most powerful model in the Qwen3 series, pushing the boundaries of what's possible with large language models and setting new performance benchmarks.
- ğŸ“– [Kimi K2.5 Announcement](https://www.kimi.com/blog/kimi-k2-5.html)
- ğŸ“– [Qwen3-Max-Thinking Announcement](https://qwen.ai/blog?id=qwen3-max-thinking)

---

## ğŸ—£ï¸ Qwen3-TTS & LuxTTS
- **Qwen3-TTS** is a new open-source series of powerful speech generation models from Qwen, offering comprehensive support for voice cloning, voice design, and ultra-high-quality human-like speech generation.
- **LuxTTS** is a lightweight, high-quality rapid TTS voice cloning model that can generate speech at speeds exceeding 150x realtime, making it ideal for a wide range of applications.
- ğŸ“– [Qwen3-TTS Announcement](https://qwen.ai/blog?id=qwen3tts-0115)
- ğŸ”— [LuxTTS on GitHub](https://github.com/ysharma3501/LuxTTS)

---

## ğŸ‘¤ PersonaPlex & CoDance
- **PersonaPlex** is a full-duplex conversational AI model from NVIDIA that enables natural conversations with customizable voices and roles, handling interruptions and backchannels just like a human.
- **CoDance** is a new framework for generating coherent and controllable dance motions from music, a significant step forward for creating realistic and expressive digital dancers.
- ğŸ”— [PersonaPlex Project Page](https://research.nvidia.com/labs/adlr/personaplex/)
- ğŸ”— [CoDance Project Page](https://lucaria-academy.github.io/CoDance/)

---

## ğŸ§  MemoryV2V & Chroma 1.0
- **MemoryV2V** is a new video-to-video translation framework that leverages memory to maintain temporal consistency and improve the quality of generated videos.
- **Chroma 1.0** is the world's first open-source, end-to-end, real-time speech-to-speech AI model with personalized voice cloning, released by FlashLabs.
- ğŸ”— [MemoryV2V Project Page](https://dohunlee1.github.io/MemoryV2V/)
- ğŸ¤— [Chroma 1.0 on Hugging Face](https://huggingface.co/FlashLabs/Chroma-4B)

---

## ğŸ¤ VibeVoice ASR & FlowAct R1
- **VibeVoice-ASR** is a unified speech-to-text model from Microsoft that can handle up to 60 minutes of long-form audio in a single pass, generating structured transcriptions with speaker identification and timestamps.
- **FlowAct R1** is a framework for interactive humanoid video generation that can synthesize lifelike videos with naturally expressive behaviors in real-time.
- ğŸ¤— [VibeVoice ASR on Hugging Face](https://huggingface.co/microsoft/VibeVoice-ASR)
- ğŸ”— [FlowAct R1 Project Page](https://grisoon.github.io/FlowAct-R1/)

---

## ğŸ”„ OmniTransfer & Step3 VL
- **OmniTransfer** is an all-in-one framework for spatio-temporal video transfer that unifies spatial appearance and temporal video transfer tasks within a single framework.
- **Step3-VL-10B** is a compact yet powerful 10B parameter multimodal model that excels in visual perception, complex reasoning, and human-centric alignment.
- ğŸ”— [OmniTransfer Project Page](https://pangzecheung.github.io/OmniTransfer/)
- ğŸ”— [Step3 VL Project Page](https://stepfun-ai.github.io/Step3-VL-10B/)

---

## ğŸ¤– VIGA & Waypoint 1
- **VIGA** (Vision-as-Inverse-Graphics Agent) is a multimodal agent that reconstructs images as editable scene programs through an analysis-by-synthesis loop.
- **Waypoint 1** is a real-time interactive video diffusion model that can generate video game footage from control inputs and text captions.
- ğŸ”— [VIGA Project Page](https://fugtemypt123.github.io/VIGA-website/)
- ğŸ“– [Waypoint 1 Announcement](https://huggingface.co/blog/waypoint-1)

---

## ğŸ¥ VideoMama, ClawdBot, Linum V2, Motion 3to4, LightOnOCR & Motive
- **VideoMama** is a mask-guided video matting framework that leverages a video generative prior to convert coarse segmentation masks into pixel-accurate alpha mattes.
- **ClawdBot** is an open-source, self-hosted personal AI assistant that runs on your own devices and integrates with your existing messaging apps.
- **Linum V2** is a 2B parameter text-to-video model that generates 720p videos at 24 FPS from text prompts.
- **Motion 3to4** is a framework for 3D motion reconstruction for 4D synthesis, a key step towards creating realistic and controllable 4D content.
- **LightOnOCR-2-1B** is an efficient end-to-end 1B-parameter vision-language model for converting documents into clean, naturally ordered text.
- **MOTIVE** is a scalable, motion-centric data attribution framework for video generation that can identify which training clips improve or degrade motion dynamics.
- ğŸ”— [VideoMama Project Page](https://cvlab-kaist.github.io/VideoMaMa/)
- ğŸ”— [ClawdBot Website](https://www.rdj.ai/clawdbot-explained-what-it-is-how-to-get-started)
- ğŸ“– [Linum V2 Announcement](https://www.linum.ai/field-notes/launch-linum-v2)
- ğŸ”— [Motion 3to4 Project Page](https://motion3-to-4.github.io/)
- ğŸ¤— [LightOnOCR on Hugging Face](https://huggingface.co/lightonai/LightOnOCR-2-1B)
- ğŸ”— [MOTIVE Project Page](https://research.nvidia.com/labs/sil/projects/MOTIVE/)

---

## ğŸš€ Wrap Up
This week marks a significant shift from AI demos to real-world applications. We're seeing AI move into live, real-time, and autonomous use cases, with open-source models leading the charge in many areas.
- **Real-time is the new standard**: From livestreaming to speech and video generation, AI is becoming faster and more responsive than ever.
- **Open-source is thriving**: The open-source community is delivering powerful, accessible tools that are pushing the boundaries of what's possible.
- **AI is becoming more autonomous**: With tools like ClawdBot and VIGA, we're seeing the emergence of AI agents that can control tools, systems, and workflows on their own.

ğŸ‘‰ What do you think is the most exciting development this week? Are you more interested in real-time AI, the latest open-source models, or the rise of autonomous agents?

ğŸ’¬ Drop your thoughts in the video comments:
[Watch the full video on YouTube](https://youtu.be/HlDYI_G4Kb0)

---

## ğŸ”— Follow & Support
- ğŸ¦ Twitter/X: [@airesearch_ai](https://x.com/airesearch_ai)  
- â˜• Support: [Ko-fi](https://ko-fi.com/airesearchs)  
- ğŸ¥ Subscribe for more: [AI Research YouTube](https://www.youtube.com/@airesearchofficial/)

---

#AI #AINews #AIWeekly #ArtificialIntelligence #OpenSource #RealtimeAI #AILivestream #AIvideo #AIagents #AIResearch

ğŸ‘‰ Browse all past episodes here: [AI Weekly News Archive](../../..)
